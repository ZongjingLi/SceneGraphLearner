{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/melkor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 128, 3])\n",
      "torch.Size([65536]) torch.Size([65536])\n",
      "torch.Size([2198]) torch.Size([2198])\n",
      "level:0 torch.Size([4, 128, 128, 3])\n",
      "level:1 torch.Size([4, 128, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models import *\n",
    "from config import *\n",
    "\n",
    "from datasets import *\n",
    "\n",
    "import math\n",
    "\n",
    "dataset = ToyData(\"train\")\n",
    "\n",
    "idx = [4, 7, 9, 11]\n",
    "ims = torch.cat([dataset[i][\"image\"].unsqueeze(0) for i in idx])\n",
    "\n",
    "print(ims.shape)\n",
    "\n",
    "percept = PSGNet(config.imsize,3)\n",
    "\n",
    "\n",
    "outputs = percept(ims)\n",
    "\n",
    "def normalize_outputs(outputs):\n",
    "    recons = outputs[\"recons\"]\n",
    "    B = recons[0].shape[0]\n",
    "    W = int(math.sqrt(recons[0].shape[1]))\n",
    "\n",
    "    clusters  = outputs[\"clusters\"]\n",
    "    features  = outputs[\"features\"]\n",
    "    centroids = outputs[\"centroids\"]\n",
    "    moments   = outputs[\"moments\"]\n",
    "\n",
    "    for item in clusters:print(item[0].shape, item[1].shape)\n",
    "\n",
    "    level_reconstructions = [item.reshape([B,W,W,3]) for item in recons]\n",
    "\n",
    "    return {\"recons\":level_reconstructions, \n",
    "    \"clusters\": clusters, \n",
    "    \"features\": features, \n",
    "    \"centroids\": centroids, \n",
    "    \"moments\": moments,\n",
    "    \"batch\":outputs[\"batch\"]}\n",
    "\n",
    "outputs = normalize_outputs(outputs)\n",
    "recons = outputs[\"recons\"]\n",
    "\n",
    "for i,item in enumerate(recons):print(\"level:{}\".format(i),item.shape)\n",
    "\n",
    "\n",
    "\n",
    "class AbstractNet(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads =8\n",
    "        self.feature_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = config.global_feature_dim,batch_first = True)\n",
    "        self.spatial_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = config.global_feature_dim,batch_first = True)\n",
    "        self.source_heads = nn.Parameter(torch.randn([self.num_heads,config.global_feature_dim]))\n",
    "\n",
    "        self.coordinate_decoder = nn.Linear(config.global_feature_dim, 2)\n",
    "    \n",
    "    def forward(self, feature, spatial):\n",
    "        B, M, C = feature.shape\n",
    "        N = self.num_heads\n",
    "        # [Feature Propagation]\n",
    "        component_features = feature\n",
    "        component_spaitals = spatial\n",
    "\n",
    "        # [Decode Proposals]\n",
    "        global_feature = torch.randn()\n",
    "        source_heads = self.souce_heads\n",
    "        feature_proposals = self.feature_decoder(source_heads,global_feature)\n",
    "        spaital_proposals = self.spatial_decoder(source_heads,global_feature)\n",
    "\n",
    "        # [Component Matching]\n",
    "        # component_features : [B,M,C]\n",
    "        # feature_proposals  : [B,N,C]\n",
    "\n",
    "        match = torch.softmax(torch.einsum(\"bnc,bmc -> bnm\",component_features, proposal_features)/math.sqrt(C), dim = -1)\n",
    "        existence = torch.max(match, dim = 1).values  # [B, N, 1]\n",
    "\n",
    "        # [Construct Representation]\n",
    "        output_graph = 0\n",
    "\n",
    "        return output_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters\n",
      "torch.Size([65536]) torch.Size([65536])\n",
      "torch.Size([2198]) torch.Size([2198])\n",
      "features\n",
      "torch.Size([2198, 64])\n",
      "torch.Size([1021, 64])\n",
      "centroids\n",
      "torch.Size([2198, 2])\n",
      "torch.Size([1021, 2])\n",
      "moments\n",
      "torch.Size([2198, 2])\n",
      "torch.Size([1021, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"clusters\")\n",
    "clusters = outputs[\"clusters\"]\n",
    "for item in clusters:\n",
    "    nodes = item[0]\n",
    "    batch = item[1]\n",
    "    print(nodes.shape, batch.shape)\n",
    "\n",
    "\n",
    "print(\"features\")\n",
    "features = outputs[\"features\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n",
    "\n",
    "print(\"centroids\")\n",
    "features = outputs[\"centroids\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n",
    "print(\"moments\")\n",
    "features = outputs[\"moments\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dense_features(outputs):\n",
    "    level_batch = outputs[\"batch\"]\n",
    "    features = outputs[\"features\"]\n",
    "    centroids = outputs[\"centroids\"]\n",
    "    moments = outputs[\"moments\"]\n",
    "    level_features = []\n",
    "    for i in range(len(features)):\n",
    "        sparse_feature = features[i]\n",
    "        sparse_centroid = centroids[i]\n",
    "        sparse_moment = moments[i]\n",
    "\n",
    "\n",
    "        cast_batch = level_batch[i]\n",
    "\n",
    "        feature,  batch = to_dense_batch(features[i],cast_batch)\n",
    "        centroid, batch = to_dense_batch(centroids[i],cast_batch)\n",
    "        moment,   batch = to_dense_batch(moments[i],cast_batch)\n",
    "\n",
    "\n",
    "        level_features.append({\"features\":feature, \"centroids\":centroid, \"moments\":moment,\"masks\":batch.int()})\n",
    "    return level_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_features = to_dense_features(outputs)\n",
    "\n",
    "base_graph = psg_features[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 260])\n",
      "torch.Size([4, 260, 66]) torch.Size([4, 25, 66])\n",
      "torch.Size([4, 260, 25])\n",
      "tensor(0.3086, grad_fn=<MaxBackward1>)\n",
      "tensor(0., grad_fn=<MinBackward1>)\n",
      "tensor([[11.1965,  4.4657, 12.1617,  2.1431,  7.2188,  8.1599,  3.0704,  1.7406,\n",
      "          2.2825,  1.8195,  3.8319,  7.6983,  1.8318,  6.9286, 17.4582, 24.1367,\n",
      "          5.1656,  4.3672,  5.2796,  1.5961, 10.1041,  2.1134,  6.0262,  7.4301,\n",
      "         32.7737],\n",
      "        [ 1.9365, 20.3519,  9.6738,  1.1250,  4.8740,  7.4356,  8.5526,  3.7956,\n",
      "          3.9474,  4.1809,  7.5315,  2.1582,  3.9368,  6.7084, 12.8178,  3.0392,\n",
      "         11.4981,  4.0942,  1.5633,  4.9882,  3.0433,  1.1033,  2.9720,  3.1339,\n",
      "         49.5383],\n",
      "        [16.4136,  3.0711,  3.6567,  2.3932,  5.6319,  3.0945, 10.4118,  3.9161,\n",
      "          3.0266,  4.4475,  1.5872,  5.2647,  8.0425,  5.9415,  2.7761,  5.0957,\n",
      "          9.7911, 14.1159,  1.4987, 30.9360,  3.3030, 21.5039, 18.4708,  4.5584,\n",
      "          6.0515],\n",
      "        [ 3.8527,  1.9679,  8.8792,  7.6273, 18.0712, 16.8834,  7.6680, 13.8423,\n",
      "         23.6547,  4.0927, 10.8134, 17.0893,  4.3542,  3.1586,  8.6489,  1.1318,\n",
      "          3.2114,  2.6727, 10.1775,  1.1262,  6.3716,  0.9722,  7.7651,  7.1886,\n",
      "          9.7790]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "width = 7\n",
    "dim = 64\n",
    "\n",
    "num_heads = width\n",
    "feature_dim = dim\n",
    "        \n",
    "relation_extractor = None\n",
    "propoerty_extractor = None\n",
    "\n",
    "proposal_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = feature_dim,batch_first = True, dim_feedforward = 128)\n",
    "spatial_decoder = nn.Linear(64,2)\n",
    "feature_decoder = nn.Linear(64,64)\n",
    "\n",
    "#nn.Transformer(nhead=16, num_encoder_layers=12,d_model = feature_dim,batch_first = True, dim_feedforward = 128)\n",
    "source_heads = nn.Parameter(torch.randn([num_heads,feature_dim]))\n",
    "\n",
    "# [Test the input feature nodes]\n",
    "\n",
    "input_graph = base_graph\n",
    "\n",
    "# [Feature Propagation]\n",
    "features = input_graph[\"features\"]\n",
    "spatials = input_graph[\"centroids\"]\n",
    "masks = input_graph[\"masks\"]\n",
    "C = 1\n",
    "print(masks.shape)\n",
    "\n",
    "# [Decode Proposals]\n",
    "B = 4\n",
    "source_key = source_heads.unsqueeze(0).repeat(B,1,1)\n",
    "target_key = torch.randn([4, 25, 64])\n",
    "proposals = proposal_decoder(source_key, target_key)\n",
    "feature_proposals = feature_decoder(proposals)\n",
    "spatial_proposals = spatial_decoder(proposals)\n",
    "\n",
    "# [Component Matching]\n",
    " # component_features : [B,M,C]\n",
    "component_features = torch.cat([features, spatials], -1)\n",
    "\n",
    "# feature_proposals  : [B,N,C]\n",
    "proposal_features = torch.cat([feature_proposals,spatial_proposals], -1)\n",
    "\n",
    "print(component_features.shape, proposal_features.shape)\n",
    "\n",
    "match = torch.softmax(torch.einsum(\"bnc,bmc -> bnm\",component_features, proposal_features)/math.sqrt(C), dim = -1)\n",
    "match = match * (masks.unsqueeze(-1))\n",
    "\n",
    "\n",
    "existence = torch.max(match, dim = 1).values  # [B, N, 1]\n",
    "print(match.shape)\n",
    "# [Construct Representation]\n",
    "output_graph = {\"features\":0, \"masks\":existence, \"edge\":match}\n",
    "\n",
    "print(match.max())\n",
    "print(match.min())\n",
    "print(torch.sum(match, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259.9999\n"
     ]
    }
   ],
   "source": [
    "a = [4.1488,  5.6714, 11.6582,  8.1207,  3.8911, 10.2980, 10.6350,  4.7643,\n",
    "          5.7072,  6.0902,  7.7078, 12.6339,  3.3813, 12.5737, 24.0381, 16.0200,\n",
    "         23.8276,  5.9760, 11.7249, 20.5651,  7.2184,  5.2609,  4.2785, 28.6767,\n",
    "          5.1321]\n",
    "\n",
    "cnt = 0\n",
    "for i in a:cnt += i\n",
    "print(cnt)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a47e46093c771f9510c4aabf3710bfb1355e5f870a13f8c22092f45d4d23626d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Melkor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
