{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/melkor/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128, 128, 3])\n",
      "torch.Size([65536]) torch.Size([65536])\n",
      "torch.Size([2198]) torch.Size([2198])\n",
      "level:0 torch.Size([4, 128, 128, 3])\n",
      "level:1 torch.Size([4, 128, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from models import *\n",
    "from config import *\n",
    "\n",
    "from datasets import *\n",
    "\n",
    "import math\n",
    "\n",
    "dataset = ToyData(\"train\")\n",
    "\n",
    "idx = [4, 7, 9, 11]\n",
    "ims = torch.cat([dataset[i][\"image\"].unsqueeze(0) for i in idx])\n",
    "\n",
    "print(ims.shape)\n",
    "\n",
    "percept = PSGNet(config.imsize,3)\n",
    "\n",
    "\n",
    "outputs = percept(ims)\n",
    "\n",
    "def normalize_outputs(outputs):\n",
    "    recons = outputs[\"recons\"]\n",
    "    B = recons[0].shape[0]\n",
    "    W = int(math.sqrt(recons[0].shape[1]))\n",
    "\n",
    "    clusters  = outputs[\"clusters\"]\n",
    "    features  = outputs[\"features\"]\n",
    "    centroids = outputs[\"centroids\"]\n",
    "    moments   = outputs[\"moments\"]\n",
    "\n",
    "    for item in clusters:print(item[0].shape, item[1].shape)\n",
    "\n",
    "    level_reconstructions = [item.reshape([B,W,W,3]) for item in recons]\n",
    "\n",
    "    return {\"recons\":level_reconstructions, \n",
    "    \"clusters\": clusters, \n",
    "    \"features\": features, \n",
    "    \"centroids\": centroids, \n",
    "    \"moments\": moments,\n",
    "    \"batch\":outputs[\"batch\"]}\n",
    "\n",
    "outputs = normalize_outputs(outputs)\n",
    "recons = outputs[\"recons\"]\n",
    "\n",
    "for i,item in enumerate(recons):print(\"level:{}\".format(i),item.shape)\n",
    "\n",
    "\n",
    "\n",
    "class AbstractNet(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_heads =8\n",
    "        self.feature_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = config.global_feature_dim,batch_first = True)\n",
    "        self.spatial_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = config.global_feature_dim,batch_first = True)\n",
    "        self.source_heads = nn.Parameter(torch.randn([self.num_heads,config.global_feature_dim]))\n",
    "\n",
    "        self.coordinate_decoder = nn.Linear(config.global_feature_dim, 2)\n",
    "    \n",
    "    def forward(self, feature, spatial):\n",
    "        B, M, C = feature.shape\n",
    "        N = self.num_heads\n",
    "        # [Feature Propagation]\n",
    "        component_features = feature\n",
    "        component_spaitals = spatial\n",
    "\n",
    "        # [Decode Proposals]\n",
    "        global_feature = torch.randn()\n",
    "        source_heads = self.souce_heads\n",
    "        feature_proposals = self.feature_decoder(source_heads,global_feature)\n",
    "        spaital_proposals = self.spatial_decoder(source_heads,global_feature)\n",
    "\n",
    "        # [Component Matching]\n",
    "        # component_features : [B,M,C]\n",
    "        # feature_proposals  : [B,N,C]\n",
    "\n",
    "        match = torch.softmax(torch.einsum(\"bnc,bmc -> bnm\",component_features, proposal_features)/math.sqrt(C), dim = -1)\n",
    "        existence = torch.max(match, dim = 1).values  # [B, N, 1]\n",
    "\n",
    "        # [Construct Representation]\n",
    "        output_graph = 0\n",
    "\n",
    "        return output_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusters\n",
      "torch.Size([65536]) torch.Size([65536])\n",
      "torch.Size([2198]) torch.Size([2198])\n",
      "features\n",
      "torch.Size([2198, 64])\n",
      "torch.Size([1021, 64])\n",
      "centroids\n",
      "torch.Size([2198, 2])\n",
      "torch.Size([1021, 2])\n",
      "moments\n",
      "torch.Size([2198, 2])\n",
      "torch.Size([1021, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"clusters\")\n",
    "clusters = outputs[\"clusters\"]\n",
    "for item in clusters:\n",
    "    nodes = item[0]\n",
    "    batch = item[1]\n",
    "    print(nodes.shape, batch.shape)\n",
    "\n",
    "\n",
    "print(\"features\")\n",
    "features = outputs[\"features\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n",
    "\n",
    "print(\"centroids\")\n",
    "features = outputs[\"centroids\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n",
    "print(\"moments\")\n",
    "features = outputs[\"moments\"]\n",
    "for item in features:\n",
    "    print(item.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dense_features(outputs):\n",
    "    level_batch = outputs[\"batch\"]\n",
    "    features = outputs[\"features\"]\n",
    "    centroids = outputs[\"centroids\"]\n",
    "    moments = outputs[\"moments\"]\n",
    "    level_features = []\n",
    "    for i in range(len(features)):\n",
    "        sparse_feature = features[i]\n",
    "        sparse_centroid = centroids[i]\n",
    "        sparse_moment = moments[i]\n",
    "\n",
    "\n",
    "        cast_batch = level_batch[i]\n",
    "\n",
    "        feature,  batch = to_dense_batch(features[i],cast_batch)\n",
    "        centroid, batch = to_dense_batch(centroids[i],cast_batch)\n",
    "        moment,   batch = to_dense_batch(moments[i],cast_batch)\n",
    "\n",
    "\n",
    "        level_features.append({\"features\":feature, \"centroids\":centroid, \"moments\":moment,\"masks\":batch.int()})\n",
    "    return level_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_features = to_dense_features(outputs)\n",
    "\n",
    "base_graph = psg_features[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 260])\n",
      "torch.Size([4, 260, 66]) torch.Size([4, 25, 66])\n",
      "torch.Size([4, 260, 25])\n",
      "tensor(0.3086, grad_fn=<MaxBackward1>)\n",
      "tensor(0., grad_fn=<MinBackward1>)\n",
      "tensor([[11.1965,  4.4657, 12.1617,  2.1431,  7.2188,  8.1599,  3.0704,  1.7406,\n",
      "          2.2825,  1.8195,  3.8319,  7.6983,  1.8318,  6.9286, 17.4582, 24.1367,\n",
      "          5.1656,  4.3672,  5.2796,  1.5961, 10.1041,  2.1134,  6.0262,  7.4301,\n",
      "         32.7737],\n",
      "        [ 1.9365, 20.3519,  9.6738,  1.1250,  4.8740,  7.4356,  8.5526,  3.7956,\n",
      "          3.9474,  4.1809,  7.5315,  2.1582,  3.9368,  6.7084, 12.8178,  3.0392,\n",
      "         11.4981,  4.0942,  1.5633,  4.9882,  3.0433,  1.1033,  2.9720,  3.1339,\n",
      "         49.5383],\n",
      "        [16.4136,  3.0711,  3.6567,  2.3932,  5.6319,  3.0945, 10.4118,  3.9161,\n",
      "          3.0266,  4.4475,  1.5872,  5.2647,  8.0425,  5.9415,  2.7761,  5.0957,\n",
      "          9.7911, 14.1159,  1.4987, 30.9360,  3.3030, 21.5039, 18.4708,  4.5584,\n",
      "          6.0515],\n",
      "        [ 3.8527,  1.9679,  8.8792,  7.6273, 18.0712, 16.8834,  7.6680, 13.8423,\n",
      "         23.6547,  4.0927, 10.8134, 17.0893,  4.3542,  3.1586,  8.6489,  1.1318,\n",
      "          3.2114,  2.6727, 10.1775,  1.1262,  6.3716,  0.9722,  7.7651,  7.1886,\n",
      "          9.7790]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "width = 7\n",
    "dim = 64\n",
    "\n",
    "num_heads = width\n",
    "feature_dim = dim\n",
    "        \n",
    "relation_extractor = None\n",
    "propoerty_extractor = None\n",
    "\n",
    "proposal_decoder = nn.Transformer(nhead=16, num_encoder_layers=12,d_model = feature_dim,batch_first = True, dim_feedforward = 128)\n",
    "spatial_decoder = nn.Linear(64,2)\n",
    "feature_decoder = nn.Linear(64,64)\n",
    "\n",
    "#nn.Transformer(nhead=16, num_encoder_layers=12,d_model = feature_dim,batch_first = True, dim_feedforward = 128)\n",
    "source_heads = nn.Parameter(torch.randn([num_heads,feature_dim]))\n",
    "\n",
    "# [Test the input feature nodes]\n",
    "\n",
    "input_graph = base_graph\n",
    "\n",
    "# [Feature Propagation]\n",
    "features = input_graph[\"features\"]\n",
    "spatials = input_graph[\"centroids\"]\n",
    "masks = input_graph[\"masks\"]\n",
    "C = 1\n",
    "print(masks.shape)\n",
    "\n",
    "# [Decode Proposals]\n",
    "B = 4\n",
    "source_key = source_heads.unsqueeze(0).repeat(B,1,1)\n",
    "target_key = torch.randn([4, 25, 64])\n",
    "proposals = proposal_decoder(source_key, target_key)\n",
    "feature_proposals = feature_decoder(proposals)\n",
    "spatial_proposals = spatial_decoder(proposals)\n",
    "\n",
    "# [Component Matching]\n",
    " # component_features : [B,M,C]\n",
    "component_features = torch.cat([features, spatials], -1)\n",
    "\n",
    "# feature_proposals  : [B,N,C]\n",
    "proposal_features = torch.cat([feature_proposals,spatial_proposals], -1)\n",
    "\n",
    "print(component_features.shape, proposal_features.shape)\n",
    "\n",
    "match = torch.softmax(torch.einsum(\"bnc,bmc -> bnm\",component_features, proposal_features)/math.sqrt(C), dim = -1)\n",
    "match = match * (masks.unsqueeze(-1))\n",
    "\n",
    "\n",
    "existence = torch.max(match, dim = 1).values  # [B, N, 1]\n",
    "print(match.shape)\n",
    "# [Construct Representation]\n",
    "output_graph = {\"features\":0, \"masks\":existence, \"edge\":match}\n",
    "\n",
    "print(match.max())\n",
    "print(match.min())\n",
    "print(torch.sum(match, dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259.9999\n"
     ]
    }
   ],
   "source": [
    "a = [4.1488,  5.6714, 11.6582,  8.1207,  3.8911, 10.2980, 10.6350,  4.7643,\n",
    "          5.7072,  6.0902,  7.7078, 12.6339,  3.3813, 12.5737, 24.0381, 16.0200,\n",
    "         23.8276,  5.9760, 11.7249, 20.5651,  7.2184,  5.2609,  4.2785, 28.6767,\n",
    "          5.1321]\n",
    "\n",
    "cnt = 0\n",
    "for i in a:cnt += i\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "from torch_sparse import SparseTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GraphPropagation(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_iters=15,\n",
    "                 excite=True,\n",
    "                 inhibit=True,\n",
    "                 project=False,\n",
    "                 adj_thresh=0.5):\n",
    "        super().__init__()\n",
    "        self.num_iters = num_iters\n",
    "        self.excite = excite\n",
    "        self.inhibit = inhibit\n",
    "        self.project = project\n",
    "        self.adj_thresh = adj_thresh\n",
    "\n",
    "        self.adj_e = None\n",
    "        self.adj_i = None\n",
    "        self.norm_factor_e = None\n",
    "        self.norm_factor_i = None\n",
    "        self.activation_converge = False\n",
    "\n",
    "    def preprocess_inputs(self, h0, adj, activated):\n",
    "        B, N, K = h0.shape\n",
    "\n",
    "        # activated: binary tensor indicating nodes to start propagation from\n",
    "        if activated is None: # randomly initialized at a point\n",
    "            rand_idx = torch.randint(0, N, [B, ]).to(h0.device)\n",
    "            activated = F.one_hot(rand_idx, num_classes=N).unsqueeze(-1).float()  # [BT, N, 1]\n",
    "        else:\n",
    "            activated = activated.reshape(B, N, 1)\n",
    "\n",
    "        # create excitatory and inhibitory affinities, followed by thresholding\n",
    "        if not isinstance(adj, SparseTensor): # dense tensor\n",
    "            adj = adj.reshape(B, N, N)\n",
    "            adj_e, adj_i = adj, 1.0 - adj\n",
    "            if self.adj_thresh is not None:\n",
    "                adj_e = self._threshold(adj_e, self.adj_thresh)\n",
    "                adj_i = self._threshold(adj_i, self.adj_thresh)\n",
    "            sample_mask = None\n",
    "        else: # sparse tensor\n",
    "            adj_e = adj\n",
    "            adj_i = adj.copy().set_value_(1.0 - adj.storage.value())\n",
    "\n",
    "            sample_mask = adj_e.copy()\n",
    "            sample_mask = sample_mask.set_value_(torch.ones_like(sample_mask.storage.value()))\n",
    "\n",
    "            if self.adj_thresh is not None:\n",
    "                adj_e = self._threshold_sparse_tensor(adj_e, self.adj_thresh)\n",
    "                adj_i = self._threshold_sparse_tensor(adj_i, self.adj_thresh)\n",
    "\n",
    "        return h0, adj_e, adj_i, activated, sample_mask\n",
    "\n",
    "    def forward(self, h0, adj, activated=None):\n",
    "        \"\"\"\n",
    "        Function: Graph propagation to create the plateau map representation\n",
    "        Input:\n",
    "        - h0: initial hidden states (or equivalently, plateau map representation) with shape [B, N Q]\n",
    "        - adj: affinity matrix with shape\n",
    "        - activated: The graph propagation will start from the activated nodes\n",
    "                   It can be None or binary tensor of shape [B, N].\n",
    "                   If None, one node will be randomly selected as being activated\n",
    "        Return:\n",
    "        - plateau_map_list: a list of plateau maps of len self.num_iters. Each plateau map has shape [B, N, Q]\n",
    "        \"\"\"\n",
    "\n",
    "        h0, adj_e, adj_i, activated, sample_mask = self.preprocess_inputs(h0, adj, activated)\n",
    "\n",
    "        h = h0.clone()\n",
    "        plateau_map_list = []\n",
    "        running_activated = activated\n",
    "        self.activation_converge = False\n",
    "\n",
    "        # start graph propagation\n",
    "        for it in range(self.num_iters):\n",
    "            h, activated, running_activated = \\\n",
    "                self.propagate(h, adj_e, adj_i, activated, running_activated, sample_mask, it)\n",
    "            plateau_map_list.append(h.reshape(h0.shape))\n",
    "\n",
    "        return plateau_map_list\n",
    "\n",
    "    def propagate(self, h, adj_e, adj_i, activated, running_activated, sample_mask, iter):\n",
    "        B, N, D = h.shape\n",
    "\n",
    "        # Graph propagation starts at a subset of activated nodes\n",
    "        # If self.activation_converge is False, i.e. not all nodes are activated, \\\n",
    "        #   we need to apply masking to the affinities and compute the normalization factor accordingly.\n",
    "        # We do so until all the nodes are activated, i.e.  self.activation_converge == True\n",
    "\n",
    "        if not self.activation_converge:\n",
    "            if isinstance(adj_e, SparseTensor):\n",
    "                # apply the activation mask on the affinity tensors\n",
    "                adj_e = adj_e.mul(activated.flatten()[None])\n",
    "                adj_i = adj_i.mul(activated.flatten()[None])\n",
    "                sample_mask = sample_mask.mul(activated.flatten()[None])\n",
    "\n",
    "            # compute the normalization factors\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                norm_factor_e = torch.sum(adj_e.abs() * activated, dim=-2, keepdim=True).clamp(min=1.0).detach()\n",
    "                norm_factor_i = torch.sum(adj_i.abs() * activated, dim=-2, keepdim=True).clamp(min=1.0).detach()\n",
    "            else:\n",
    "                norm_factor_e = adj_e.sum(1).reshape(B, N, 1).clamp(min=1.0).detach()\n",
    "                norm_factor_i = adj_i.sum(1).reshape(B, N, 1).clamp(min=1.0).detach()\n",
    "\n",
    "            self.norm_factor_e = norm_factor_e # [B,1,N]\n",
    "            self.norm_factor_i = norm_factor_i # [B,1,N]\n",
    "            self.adj_e = adj_e\n",
    "            self.adj_i = adj_i\n",
    "\n",
    "            self.activation_converge = activated.sum() == (B * N)\n",
    "        else: # no update is required if all the nodes are activated\n",
    "            adj_e = self.adj_e\n",
    "            adj_i = self.adj_i\n",
    "            norm_factor_e = self.norm_factor_e\n",
    "            norm_factor_i = self.norm_factor_i\n",
    "            sample_mask = None\n",
    "\n",
    "        # [Excitation]\n",
    "        if self.excite:\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                e_effects = torch.matmul(h.permute(0, 2, 1), adj_e * activated) / norm_factor_e\n",
    "                e_effects = e_effects.permute(0, 2, 1)\n",
    "            else:\n",
    "                e_effects = adj_e.matmul(h.reshape(B * N, D))\n",
    "                e_effects = e_effects.reshape(B, N, D) / norm_factor_e\n",
    "            h = h + e_effects\n",
    "\n",
    "        # [Inhibition]\n",
    "        if self.inhibit:\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                i_effects = torch.matmul(h.permute(0, 2, 1), adj_i * activated) / norm_factor_i\n",
    "                i_effects = i_effects.permute(0, 2, 1)\n",
    "            else:\n",
    "                i_effects = adj_i.matmul(h.reshape(B * N, D))\n",
    "                i_effects = i_effects.reshape(B, N, D) / norm_factor_i\n",
    "\n",
    "            proj = self._projection(h, i_effects) if self.project else i_effects\n",
    "            h = h - proj\n",
    "\n",
    "        h = self._relu_norm(h)\n",
    "\n",
    "        # [Update activated nodes]\n",
    "        if activated.sum() < B * N:\n",
    "            if not isinstance(adj_e, SparseTensor):\n",
    "                receivers = torch.max(torch.where(adj_e > adj_i, adj_e, adj_i) * activated, dim=1, keepdim=False)[0] > 0.5 # [B,N]\n",
    "            else:\n",
    "                assert sample_mask is not None\n",
    "                receivers = sample_mask.max(dim=1) > 0.5\n",
    "                receivers = receivers.reshape(B, N)\n",
    "\n",
    "            running_activated = running_activated + receivers.unsqueeze(-1).float()\n",
    "            activated = running_activated.clamp(max=1.0).detach()\n",
    "\n",
    "        return h, activated, running_activated\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold(x, thresh):\n",
    "        return x * (x > thresh).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def _threshold_sparse_tensor(x, thresh):\n",
    "        row, col, value = x.coo()\n",
    "        valid = value > thresh\n",
    "        sparse_size = [x.size(0), x.size(1)]\n",
    "        output = SparseTensor(row=row[valid],col=col[valid],value=value[valid],sparse_sizes=sparse_size)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def _projection(v, u, eps=1e-12):\n",
    "        u_norm = torch.sum(u * u, -1, keepdims=True)\n",
    "        dot_prod = torch.sum(v * u, -1, keepdims=True)\n",
    "        proj = (dot_prod / (u_norm + eps)) * u\n",
    "        return proj\n",
    "\n",
    "    @staticmethod\n",
    "    def _relu_norm(x, relu=True, norm=True, eps=1e-16):\n",
    "        x = F.relu(x) if relu else x\n",
    "        x = F.normalize(x + eps, p=2.0, dim=-1, eps=max([eps, 1e-12])) if norm else x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "propagator = GraphPropagation(7)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a47e46093c771f9510c4aabf3710bfb1355e5f870a13f8c22092f45d4d23626d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Melkor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
