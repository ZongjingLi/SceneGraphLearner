{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "B = 2\n",
    "shuffle = 1\n",
    "#dataset = ToyDataWithQuestions(split = \"train\", resolution = (128,128))\n",
    "dataset = SpriteWithQuestions(resolution = (128,128))\n",
    "dataloader = DataLoader(dataset, batch_size = B, shuffle = shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28da186a0>"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Get A Sample Data]\n",
    "for sample in dataloader:\n",
    "    sample = sample\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(sample[\"image\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': ['not generated yet.',\n",
       "   'not generated yet.',\n",
       "   'not generated yet.',\n",
       "   'not generated yet.'],\n",
       "  'program': ['exist(filter(scene(),Circle))',\n",
       "   'exist(filter(scene(),Cube))',\n",
       "   'exist(filter(scene(),Diamond))',\n",
       "   'count(scene())'],\n",
       "  'answer': ['False', 'True', 'True', '2']},\n",
       " {'question': ['not generated yet.',\n",
       "   'not generated yet.',\n",
       "   'not generated yet.',\n",
       "   'not generated yet.'],\n",
       "  'program': ['exist(filter(scene(),Circle))',\n",
       "   'exist(filter(scene(),Cube))',\n",
       "   'exist(filter(scene(),Diamond))',\n",
       "   'count(scene())'],\n",
       "  'answer': ['True', 'True', 'True', '3']}]"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collect_qa_batch(batch_qa):\n",
    "    batch_wise_qa = []\n",
    "    \n",
    "    for b in range(len(batch_qa[0][\"program\"])):\n",
    "        questions_in_batch = {\"question\":[],\"program\":[],\"answer\":[]}\n",
    "        for qpair in batch_qa:\n",
    "\n",
    "            try:questions_in_batch[\"question\"].append(qpair[\"question\"][b])\n",
    "            except: questions_in_batch[\"question\"].append(\"not generated yet.\")\n",
    "            questions_in_batch[\"program\"].append(qpair[\"program\"][b])\n",
    "            questions_in_batch[\"answer\"].append(qpair[\"answer\"][b])\n",
    "        batch_wise_qa.append(questions_in_batch)\n",
    "    return batch_wise_qa\n",
    "\n",
    "collect_qa_batch(sample[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree(scores, connections, scale = 1.2):\n",
    "    fig = plt.figure(\"tree-visualize\",frameon = False)\n",
    "    plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "    plt.axis(\"off\")\n",
    "    x_locs = []; y_locs = []\n",
    "    for i,score in enumerate(reversed(scores)):\n",
    "        num_nodes = len(score)\n",
    "        # calculate scores each node\n",
    "        #print(score.sigmoid())\n",
    "        #score = (score.sigmoid() + 0.5).int()\n",
    "        #scores = score.sigmoid()\n",
    "        score = torch.clamp(score,0.05,1)\n",
    "\n",
    "        y_positions = [-scale*(i+1) / 2.0] * num_nodes\n",
    "        x_positions = np.linspace(-scale**(i+1), scale**(i+1), num_nodes)\n",
    "        if num_nodes == 1: x_positions = [0.0]\n",
    "        x_locs.append(x_positions); y_locs.append(y_positions)\n",
    "        \n",
    "        plt.scatter(x_positions, y_positions, alpha = score, color = 'white', linewidths=2.0)\n",
    "    for k,connection in enumerate(reversed(connections)):\n",
    "        connection = connection\n",
    "        \n",
    "        lower_node_num = len(x_locs[k])\n",
    "        upper_node_num = len(x_locs[k+1])\n",
    "        for i in range(lower_node_num):\n",
    "            for j in range(upper_node_num):\n",
    "                plt.plot( [x_locs[k][i],x_locs[k+1][j]], [y_locs[k][i], y_locs[k+1][j]], color = \"white\" ,alpha = float(connection[j][i]))\n",
    "    plt.tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "\n",
    "def answer_distribution_binary(score, name = \"answer_distribution\"):\n",
    "    batch_size = 1\n",
    "    score_size = 4\n",
    "\n",
    "    row = batch_size * score_size \n",
    "    col = row / 2\n",
    "\n",
    "    scores = [score, 1 - score]\n",
    "\n",
    "    plt.figure(\"dists\", frameon = False, figsize = (row,col))\n",
    "    plt.tick_params(left = True, right = False , labelleft = True ,\n",
    "                labelbottom = True, bottom = True)\n",
    "    plt.cla()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(1,batch_size,i + 1,frameon=False)\n",
    "        plt.cla()\n",
    "\n",
    "        \n",
    "        keys = [\"yes\",\"no\"]\n",
    "        plt.bar(keys,scores)\n",
    "        plt.tick_params(left = True, right = False , labelleft = True ,\n",
    "                labelbottom = True, bottom = True)\n",
    "\n",
    "    plt.savefig(\"outputs/{}.png\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.perception = \"valkyr\"\n",
    "model = SceneLearner(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pose(x, att):\n",
    "    x = x.permute(0,2,1)\n",
    "    # x: B3N, att: B1KN1\n",
    "    # ts: B3k1\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    att = att / torch.clamp(pai, min=1e-3)\n",
    "    ts = torch.sum(\n",
    "        att * x[:, :, None, :, None], dim=3) # B3K1\n",
    "    return ts\n",
    "\n",
    "def equillibrium_loss(att):\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    loss_att_amount = torch.var(pai.reshape(pai.shape[0], -1), dim=1).mean()\n",
    "    return loss_att_amount\n",
    "\n",
    "\n",
    "def spatial_variance(x, att, norm_type=\"l2\"):\n",
    "    \n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    att = att / torch.clamp(pai, min=1e-3)\n",
    "    ts = torch.sum(\n",
    "        att * x[:, :, None, :, None], dim=3) # B3K1\n",
    "\n",
    "    x_centered = x[:, :, None] - ts # B3KN\n",
    "    x_centered = x_centered.permute(0, 2, 3, 1) # BKN3\n",
    "    att = att.squeeze(1) # BKN1\n",
    "    cov = torch.matmul(\n",
    "        x_centered.transpose(3, 2), att * x_centered) # BK33\n",
    "    \n",
    "    # l2 norm\n",
    "    vol = torch.diagonal(cov, dim1=-2, dim2=-1).sum(2) # BK\n",
    "    if norm_type == \"l2\":\n",
    "        vol = vol.norm(dim=1).mean()\n",
    "    elif norm_type == \"l1\":\n",
    "        vol = vol.sum(dim=1).mean()\n",
    "    else:\n",
    "        # vol, _ = torch.diagonal(cov, dim1=-2, dim2=-1).sum(2).max(dim=1)\n",
    "        raise NotImplementedError\n",
    "    return vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def evaluate_pose(x, att):\n",
    "    # x: B3N, att: B1KN1\n",
    "    # ts: B3k1\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    att = att / torch.clamp(pai, min=1e-3)\n",
    "    ts = torch.sum(\n",
    "        att * x[:, :, None, :, None], dim=3) # B3K1\n",
    "    return ts\n",
    "\n",
    "def equillibrium_loss(att):\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    loss_att_amount = torch.var(pai.reshape(pai.shape[0], -1), dim=1).mean()\n",
    "    return loss_att_amount\n",
    "\n",
    "\n",
    "def spatial_variance(x, att, norm_type=\"l2\"):\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    att = att / torch.clamp(pai, min=1e-3)\n",
    "    ts = torch.sum(\n",
    "        att * x[:, :, None, :, None], dim=3) # B3K1\n",
    "\n",
    "    x_centered = x[:, :, None] - ts # B3KN\n",
    "    x_centered = x_centered.permute(0, 2, 3, 1) # BKN3\n",
    "    att = att.squeeze(1) # BKN1\n",
    "    cov = torch.matmul(\n",
    "        x_centered.transpose(3, 2), att * x_centered) # BK33\n",
    "    \n",
    "    # l2 norm\n",
    "    vol = torch.diagonal(cov, dim1=-2, dim2=-1).sum(2) # BK\n",
    "    if norm_type == \"l2\":\n",
    "        vol = vol.norm(dim=1).mean()\n",
    "    elif norm_type == \"l1\":\n",
    "        vol = vol.sum(dim=1).mean()\n",
    "    else:\n",
    "        # vol, _ = torch.diagonal(cov, dim1=-2, dim2=-1).sum(2).max(dim=1)\n",
    "        raise NotImplementedError\n",
    "    return vol\n",
    "\n",
    "class RDB_Conv(nn.Module):\n",
    "    def __init__(self, inChannels, growRate, kSize=3):\n",
    "        super(RDB_Conv, self).__init__()\n",
    "        Cin = inChannels\n",
    "        G  = growRate\n",
    "        self.conv = nn.Sequential(*[\n",
    "            nn.Conv2d(Cin, G, kSize, padding=(kSize-1)//2, stride=1),\n",
    "            nn.ReLU(),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return torch.cat((x, out), 1)\n",
    "\n",
    "class RDB(nn.Module):\n",
    "    def __init__(self, growRate0, growRate, nConvLayers, kSize=3):\n",
    "        super(RDB, self).__init__()\n",
    "        G0 = growRate0\n",
    "        G  = growRate\n",
    "        C  = nConvLayers\n",
    "\n",
    "        convs = []\n",
    "        for c in range(C):\n",
    "            convs.append(RDB_Conv(G0 + c*G, G))\n",
    "        self.convs = nn.Sequential(*convs)\n",
    "\n",
    "        # Local Feature Fusion\n",
    "        self.LFF = nn.Conv2d(G0 + C*G, G0, 1, padding=0, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.LFF(self.convs(x)) + x\n",
    "\n",
    "class RDN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(RDN, self).__init__()\n",
    "        self.args = args\n",
    "        r = args.scale[0]\n",
    "        G0 = args.G0\n",
    "        kSize = args.RDNkSize\n",
    "\n",
    "        # number of RDB blocks, conv layers, out channels\n",
    "        self.D, C, G = args.RDNconfig\n",
    "        \"\"\"\n",
    "        {\n",
    "            'A': (20, 6, 32),\n",
    "            'B': (16, 8, 64),\n",
    "        }[args.RDNconfig]\n",
    "        \"\"\"\n",
    "\n",
    "        # Shallow feature extraction net\n",
    "        self.SFENet1 = nn.Conv2d(args.n_colors, G0, kSize, padding=(kSize-1)//2, stride=1)\n",
    "        self.SFENet2 = nn.Conv2d(G0, G0, kSize, padding=(kSize-1)//2, stride=1)\n",
    "\n",
    "        # Redidual dense blocks and dense feature fusion\n",
    "        self.RDBs = nn.ModuleList()\n",
    "        for i in range(self.D):\n",
    "            self.RDBs.append(\n",
    "                RDB(growRate0 = G0, growRate = G, nConvLayers = C)\n",
    "            )\n",
    "\n",
    "        # Global Feature Fusion\n",
    "        self.GFF = nn.Sequential(*[\n",
    "            nn.Conv2d(self.D * G0, G0, 1, padding=0, stride=1),\n",
    "            nn.Conv2d(G0, G0, kSize, padding=(kSize-1)//2, stride=1)\n",
    "        ])\n",
    "\n",
    "        if args.no_upsampling:\n",
    "            self.out_dim = G0\n",
    "        else:\n",
    "            self.out_dim = args.n_colors\n",
    "            # Up-sampling net\n",
    "            if r == 2 or r == 3:\n",
    "                self.UPNet = nn.Sequential(*[\n",
    "                    nn.Conv2d(G0, G * r * r, kSize, padding=(kSize-1)//2, stride=1),\n",
    "                    nn.PixelShuffle(r),\n",
    "                    nn.Conv2d(G, args.n_colors, kSize, padding=(kSize-1)//2, stride=1)\n",
    "                ])\n",
    "            elif r == 4:\n",
    "                self.UPNet = nn.Sequential(*[\n",
    "                    nn.Conv2d(G0, G * 4, kSize, padding=(kSize-1)//2, stride=1),\n",
    "                    nn.PixelShuffle(2),\n",
    "                    nn.Conv2d(G, G * 4, kSize, padding=(kSize-1)//2, stride=1),\n",
    "                    nn.PixelShuffle(2),\n",
    "                    nn.Conv2d(G, args.n_colors, kSize, padding=(kSize-1)//2, stride=1)\n",
    "                ])\n",
    "            else:\n",
    "                raise ValueError(\"scale must be 2 or 3 or 4.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        f__1 = self.SFENet1(x)\n",
    "        x  = self.SFENet2(f__1)\n",
    "\n",
    "        RDBs_out = []\n",
    "        for i in range(self.D):\n",
    "            x = self.RDBs[i](x)\n",
    "            RDBs_out.append(x)\n",
    "\n",
    "        x = self.GFF(torch.cat(RDBs_out,1))\n",
    "        x += f__1\n",
    "\n",
    "        if self.args.no_upsampling:\n",
    "            return x\n",
    "        else:\n",
    "            return self.UPNet(x)\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "\n",
    "    def __init__(self, input_feature_num, output_feature_num, add_bias=True, dtype=torch.float,\n",
    "                 batch_normal=True):\n",
    "        super().__init__()\n",
    "        # shapes\n",
    "\n",
    "        self.input_feature_num = input_feature_num\n",
    "        self.output_feature_num = output_feature_num\n",
    "        self.add_bias = add_bias\n",
    "        self.batch_normal = batch_normal\n",
    "\n",
    "        # params\n",
    "        latent_dim = 128\n",
    "        self.weight = FCBlock(128,2,input_feature_num, latent_dim)\n",
    "        self.bias = nn.Parameter(torch.randn(latent_dim,dtype=dtype))\n",
    "        self.transform = FCBlock(128,2,latent_dim, self.output_feature_num)\n",
    "        \n",
    "        self.sparse = True\n",
    "        #self.batch_norm = nn.BatchNorm1d(num_features = input_feature_num)\n",
    "            \n",
    "    def set_trainable(self, train=True):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = train\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        @param inp : adjacent: (batch, graph_num, graph_num) cat node_feature: (batch, graph_num, in_feature_num) -> (batch, graph_num, graph_num + in_feature_num)\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        B, N, D = x.shape\n",
    "        node_feature = x\n",
    "\n",
    "        x = self.weight(node_feature)\n",
    "        #x = torch.nn.functional.normalize(x,p = 1.0, dim = -1, eps = 1e-5)\n",
    "\n",
    "        if self.sparse or isinstance(adj, torch.SparseTensor):\n",
    "            x = torch.spmm(adj,x[0]).unsqueeze(0)\n",
    "        else:\n",
    "            x = torch.matmul(adj,x[0])\n",
    "        #if self.add_bias:\n",
    "        x = x + self.bias.unsqueeze(0).unsqueeze(0).repeat(B,N,1)\n",
    "\n",
    "        x = self.transform(x)\n",
    "\n",
    "        #x = torch.nn.functional.normalize(x,p = 1.0, dim = -1, eps = 1e-5)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GNNSoftPooling(nn.Module):\n",
    "    def __init__(self, input_feat_dim, output_node_num = 10):\n",
    "        super().__init__()\n",
    "        self.assignment_net = GraphConvolution(input_feat_dim, output_node_num)\n",
    "        self.feature_net =   GraphConvolution(input_feat_dim, input_feat_dim) \n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        B,N,D = x.shape\n",
    "        # B,N,N = adj.shape\n",
    "        if isinstance(adj, list):\n",
    "            output_node_features = []\n",
    "            output_new_adj = []\n",
    "            output_s_matrix = []\n",
    "            scale = 50\n",
    "            for i in range(len(adj)):\n",
    "                s_matrix = self.assignment_net(x[i:i+1], adj[i]) #[B,N,M]\n",
    "                \n",
    "                s_matrix = torch.softmax(s_matrix * scale , dim = 2)#.clamp(0.0+eps,1.0-eps)\n",
    "            \n",
    "                node_features = self.feature_net(x[i:i+1],adj[i]) #[B,N,D]\n",
    "                node_features = torch.einsum(\"bnm,bnd->bmd\",s_matrix,node_features) #[B,M,D]\n",
    "                # [Calculate New Cluster Adjacency]\n",
    "\n",
    "                adj[i] = adj[i]\n",
    "                #print(\"smt,adj\",s_matrix.max(),s_matrix.min(), adj[i].max(), adj[i].min())\n",
    "                #print(adj[i].shape, s_matrix.shape)\n",
    "                new_adj = torch.spmm(\n",
    "                    torch.spmm(\n",
    "                        s_matrix[0].permute(1,0),adj[i]\n",
    "                        ),s_matrix[0])\n",
    "                new_adj = new_adj / new_adj.max()\n",
    "                #print(\"new_adj\",new_adj[i].max(), new_adj[i].min())\n",
    "\n",
    "                output_node_features.append(node_features)\n",
    "                output_new_adj.append(new_adj)\n",
    "                output_s_matrix.append(s_matrix)\n",
    "\n",
    "            output_node_features = torch.cat(output_node_features, dim = 0)\n",
    "            output_s_matrix = torch.cat(output_s_matrix, dim = 0)\n",
    "        return output_node_features,output_new_adj,output_s_matrix\n",
    "\n",
    "def get_fourier_feature(grid, term = 7):\n",
    "    output_feature = []\n",
    "    for k in range(term):\n",
    "        output_feature.append(torch.sin(grid * (k + 1)))\n",
    "        output_feature.append(torch.cos(grid * (k + 1)))\n",
    "    output_feature = torch.cat(output_feature, dim = -1)\n",
    "    return output_feature\n",
    "\n",
    "class ObjectRender(nn.Module):\n",
    "    def __init__(self,config, conv_feature_dim):\n",
    "        super().__init__()\n",
    "        channel_dim = config.channel\n",
    "        spatial_dim = config.spatial_dim\n",
    "        fourier_dim = config.fourier_dim\n",
    "\n",
    "        self.conv_feature_dim = conv_feature_dim\n",
    "        self.render_block  = FCBlock(128,2,conv_feature_dim + spatial_dim + spatial_dim + 2*spatial_dim*fourier_dim,channel_dim)\n",
    "\n",
    "    def forward(self, latent, grid):\n",
    "        B,N,D = latent.shape\n",
    "        if len(grid.shape) == 4:\n",
    "            # grid: [B,W,H,2]\n",
    "            B, W, H, _ = grid.shape\n",
    "            expand_latent = latent.unsqueeze(2).unsqueeze(2)\n",
    "            expand_latent = expand_latent.repeat(1,1,W,H,1)\n",
    "            grid = grid.unsqueeze(1)\n",
    "            grid = grid.repeat(1,N,1,1,1)\n",
    "        if len(grid.shape) == 3:\n",
    "            # grid: [B,WH,2]\n",
    "            B, WH, _ = grid.shape\n",
    "            grid = grid.unsqueeze(1).repeat(1,N,1,1)\n",
    "            expand_latent = latent.unsqueeze(2).repeat(1,1,WH,1)\n",
    "        cat_feature = torch.cat([grid, expand_latent], dim = -1)\n",
    "        return self.render_block(cat_feature)\n",
    "\n",
    "\n",
    "class ValkyrNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        device = config.device\n",
    "        # construct the grid domain connection\n",
    "        self.imsize = config.imsize\n",
    "        self.perception_size = config.perception_size\n",
    "        # build the connection graph for the grid domain\n",
    "        self.spatial_coords = grid(self.imsize,self.imsize,device=device)\n",
    "        self.spatial_fourier_features = get_fourier_feature(self.spatial_coords, term = config.fourier_dim).to(device)\n",
    "        self.spatial_edges =  build_perception(self.imsize,self.perception_size,device = device).to_dense().to(device)\n",
    "        # [Grid Convs]\n",
    "        conv_feature_dim = config.conv_feature_dim\n",
    "        self.grid_convs = RDN(SimpleNamespace(G0=conv_feature_dim  ,RDNkSize=3,n_colors=3,RDNconfig=(4,3,16),scale=[2],no_upsampling=True))\n",
    "        \n",
    "        # [Diff Pool Construction]\n",
    "        hierarchy_nodes = config.hierarchy_construct \n",
    "        self.diff_pool = nn.ModuleList([\n",
    "            GNNSoftPooling(input_feat_dim = conv_feature_dim+2,output_node_num = node_num ) for node_num in hierarchy_nodes\n",
    "        ])\n",
    "        \n",
    "\n",
    "        # [Render Fields]\n",
    "        self.render_fields = nn.ModuleList([ObjectRender(config, conv_feature_dim) for _ in hierarchy_nodes])\n",
    "\n",
    "        self.conv2object_feature = nn.Linear(conv_feature_dim + 2, config.object_dim)\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    def forward(self, x, verbose = 0):\n",
    "        outputs = {}\n",
    "        B,W,H,C = x.shape # input shape\n",
    "        device = self.device\n",
    "\n",
    "        # [Grid Convolution] produce initial feature in the grid domain \n",
    "        grid_conv_feature = self.grid_convs(x.permute(0,3,1,2)).to(device).permute(0,2,3,1)\n",
    "\n",
    "        _,_,_,D = grid_conv_feature.shape\n",
    "        coords_added_conv_feature = torch.cat(\n",
    "            [grid_conv_feature, self.spatial_coords.unsqueeze(0).repeat(B,1,1,1).to(device)], dim = 3\n",
    "        )\n",
    "        if verbose:print(\"coords_added_conv_feature:{}x{}x{}x{}\".format(*list(coords_added_conv_feature.shape) ))\n",
    "\n",
    "        coords_added_conv_feature = coords_added_conv_feature.reshape(B,W*H,(D+2))\n",
    "        coords_added_conv_feature = F.normalize(coords_added_conv_feature, dim = 2, p=1.0)\n",
    "       \n",
    "        # [DiffPool] each layer performs differentiable [Pn invariant] pooling \n",
    "\n",
    "        convs_features = []\n",
    "        cluster_assignments = []\n",
    "        curr_x = coords_added_conv_feature # base layer feature\n",
    "  \n",
    "        curr_edges = [self.spatial_edges for _ in range(B)] # base layer edges\n",
    "        convs_features.append(curr_x)\n",
    "        entropy_regular = 0.0 # initialize the entropy loss\n",
    "        loc_loss = 0.0        # localization loss\n",
    "        equi_loss = 0.0       # equillibrium loss\n",
    "        scene_tree = {\n",
    "            \"x\":[curr_x],\n",
    "            \"object_features\":[self.conv2object_feature(curr_x)],\n",
    "            \"object_scores\":[torch.ones(B,curr_x.shape[1]).to(self.device)],\n",
    "            \"connections\":[],\n",
    "            \"edges\":[self.spatial_edges]}\n",
    "        outputs[\"masks\"] = []\n",
    "        outputs[\"poses\"] = []\n",
    "\n",
    "        layer_reconstructions = []\n",
    "        layer_masks = [torch.ones(B,curr_x.shape[1]).to(self.device)]  # maintain a mask\n",
    "        for i,graph_pool in enumerate(self.diff_pool):\n",
    "            curr_x, curr_edges, assignment_matrix = graph_pool(curr_x, curr_edges)\n",
    "            B,N,M = assignment_matrix.shape\n",
    "            assignment_matrix = scene_tree[\"object_scores\"][-1].unsqueeze(2).repeat(1,1,M) * assignment_matrix\n",
    "            #assignment_matrix = F.normalize(assignment_matrix, dim = 2)\n",
    "\n",
    "            # previous level mask calculation\n",
    "            prev_mask = layer_masks[-1]\n",
    "            #print(prev_mask.shape, assignment_matrix.shape)\n",
    "            if len(prev_mask.shape) == 2:\n",
    "                layer_mask = assignment_matrix #[BxNxWxHx1]\n",
    "            else:layer_mask = torch.bmm(prev_mask,assignment_matrix)\n",
    "\n",
    "\n",
    "            layer_masks.append(layer_mask)\n",
    "                        \n",
    "            exist_prob = torch.max(assignment_matrix,dim = 1).values\n",
    "            #exist_prob = torch.ones(B, assignment_matrix.shape[-1]).to(device)\n",
    "\n",
    "            # [Equivariance Loss]\n",
    "            equis =assignment_matrix.unsqueeze(1).unsqueeze(-1)\n",
    "            equi_loss += equillibrium_loss(equis)\n",
    "            \n",
    "            cluster_assignments.append(assignment_matrix)\n",
    "            convs_features.append(curr_x)\n",
    "\n",
    "            # [Scene Reconstruction]\n",
    "            syn_grid = torch.cat([self.spatial_coords.to(device)\\\n",
    "                                  ,self.spatial_fourier_features.to(device)], dim = -1).unsqueeze(0).repeat(B,1,1,1)\n",
    "\n",
    "            layer_recons = self.render_fields[i](\n",
    "                curr_x,\n",
    "                syn_grid\n",
    "                )\n",
    "\n",
    "            if verbose: print(\"reconstruction with shape: \", layer_recons.shape)\n",
    "            layer_reconstructions.append(layer_recons)\n",
    "            \n",
    "            if verbose:print(assignment_matrix.max(),assignment_matrix.min(), curr_edges[0].shape, curr_edges[0].max(), curr_edges[0].min())\n",
    "            \n",
    "            # [Regular Entropy Term]\n",
    "            \n",
    "            attention_mask = layer_mask\n",
    "            outputs[\"masks\"].append(attention_mask)\n",
    "            \n",
    "            points = self.spatial_coords.unsqueeze(0).repeat(B,1,1,1).reshape(B,W*H,2)\n",
    "            #print(attention_mask.shape, points.shape)\n",
    "            #pose_locals = evaluate_pose(points , attention_mask)\n",
    "            variance = spatial_variance(points, attention_mask.permute(0,2,1), norm_type=\"l2\")\n",
    "            loc_loss += variance\n",
    "            #equi_loss += equillibrium_loss(attention_mask)\n",
    "\n",
    "            # [Poses]\n",
    "\n",
    "            poses = evaluate_pose(points,layer_mask.permute(0,2,1))\n",
    "            outputs[\"poses\"].append({\"centers\":poses,\"vars\":variance})\n",
    "\n",
    "            entropy_regular += assignment_entropy(assignment_matrix)\n",
    "\n",
    "            # load results to the scene tree\n",
    "            scene_tree[\"x\"].append(curr_x)\n",
    "            scene_tree[\"object_features\"].append(self.conv2object_feature(curr_x))\n",
    "            scene_tree[\"object_scores\"].append(exist_prob)\n",
    "            scene_tree[\"connections\"].append(assignment_matrix)\n",
    "            scene_tree[\"edges\"].append(curr_edges)\n",
    "\n",
    "        # [Calculate Reconstruction at Each Layer]\n",
    "        outputs[\"reconstructions\"] = []\n",
    "\n",
    "        reconstruction_loss = 0.0\n",
    "\n",
    "        for i,recons in enumerate(layer_reconstructions):\n",
    "\n",
    "            B,N,W,H,C = recons.shape\n",
    "\n",
    "            exist_prob = scene_tree[\"object_scores\"][i+1]\\\n",
    "                .unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1,1,W,H,C) \n",
    "\n",
    "            mask = layer_masks[i+1].permute(0,2,1).reshape(B,N,W,H,1)\n",
    "\n",
    "            recons = recons * mask * exist_prob\n",
    "            \n",
    "            #layer_recon_loss = torch.nn.functional.mse_loss(recons, x.unsqueeze(1).repeat(1,N,1,1,1))\n",
    "            layer_recon_loss = torch.nn.functional.mse_loss(recons.sum(dim = 1), x)\n",
    "            reconstruction_loss += layer_recon_loss\n",
    "            outputs[\"reconstructions\"].append(recons)\n",
    "\n",
    "\n",
    "        # [Output the Scene Tree]\n",
    "        outputs[\"scene_tree\"] = scene_tree\n",
    "\n",
    "        # [Add all the loss terms]\n",
    "        outputs[\"losses\"] = {\"entropy\":entropy_regular,\"reconstruction\":reconstruction_loss,\"equi\":equi_loss,\"localization\":loc_loss}\n",
    "        return outputs\n",
    "\n",
    "def evaluate_pose(x, att):\n",
    "    # x: BN3, att: BKN\n",
    "    # ts: B3k1\n",
    "    att = att.unsqueeze(1).unsqueeze(-1)\n",
    "    x = x.permute(0,2,1)\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    att = att / torch.clamp(pai, min=1e-3)\n",
    "    ts = torch.sum(\n",
    "        att * x[:, :, None, :, None], dim=3) # B3K1\n",
    "    return ts.permute(0,2,1,3).squeeze(-1)\n",
    "\n",
    "def spatial_variance(x, att, norm_type=\"l2\"):\n",
    "    # att: BKN x: BN3\n",
    "    x = x.permute(0,2,1)\n",
    "    att = att.unsqueeze(1).unsqueeze(-1)\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    att = att / torch.clamp(pai, min=1e-3)\n",
    "    ts = torch.sum(\n",
    "        att * x[:, :, None, :, None], dim=3) # B3K1\n",
    "\n",
    "    x_centered = x[:, :, None] - ts # B3KN\n",
    "    x_centered = x_centered.permute(0, 2, 3, 1) # BKN3\n",
    "    att = att.squeeze(1) # BKN1\n",
    "    cov = torch.matmul(\n",
    "        x_centered.transpose(3, 2), att * x_centered) # BK33\n",
    "    \n",
    "    # l2 norm\n",
    "    vol = torch.diagonal(cov, dim1=-2, dim2=-1).sum(2) # BK\n",
    "    if norm_type == \"l2\":\n",
    "        vol = vol.norm(dim=1).mean()\n",
    "    elif norm_type == \"l1\":\n",
    "        vol = vol.sum(dim=1).mean()\n",
    "    else:\n",
    "        # vol, _ = torch.diagonal(cov, dim1=-2, dim2=-1).sum(2).max(dim=1)\n",
    "        raise NotImplementedError\n",
    "    return vol\n",
    "\n",
    "def assignment_entropy(s_matrix):\n",
    "    # s_matrix: B,N,M\n",
    "    EPS = 1e-6\n",
    "    output_entropy = 0\n",
    "    for b in range(s_matrix.shape[0]):\n",
    "        for i in range(s_matrix.shape[1]):\n",
    "            input_tensor = s_matrix[b][i:i+1,:].clamp(EPS, 1-EPS)\n",
    "\n",
    "            lsm = nn.LogSoftmax(dim = -1)\n",
    "            log_probs = lsm(input_tensor)\n",
    "            probs = torch.exp(log_probs)\n",
    "            p_log_p = log_probs * probs\n",
    "            entropy = -p_log_p.mean()\n",
    "            #print(entropy)\n",
    "            output_entropy += entropy\n",
    "    output_entropy *= 0\n",
    "    return output_entropy\n",
    "    \n",
    "\n",
    "def equillibrium_loss(att):\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    loss_att_amount = torch.var(pai.reshape(pai.shape[0], -1), dim=1).mean()\n",
    "    return loss_att_amount\n",
    "\n",
    "def build_perception(size,length,device):\n",
    "    edges = [[],[]]\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            # go for all the points on the grid\n",
    "            coord = [i,j];loc = i * size + j\n",
    "\n",
    "            for dx in range(-length,length+1):\n",
    "                for dy in range(-length,length+1):\n",
    "                    if i+dx < size and i+dx>=0 and j+dy<size and j+dy>=0:\n",
    "                        if (i+dx) * size + (j + dy) != loc:\n",
    "                            edges[0].append(loc)\n",
    "                            edges[1].append( (i+dx) * size + (j + dy))\n",
    "                            edges[0].append( (i+dx) * size + (j + dy))\n",
    "                            edges[1].append(loc)\n",
    "    outputs = torch.sparse_coo_tensor(edges, torch.ones(len(edges[0])), size = (size**2, size**2))\n",
    "    return outputs.to(device)\n",
    "\n",
    "def grid(width, height, device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    x = torch.linspace(0,1,width)\n",
    "    y = torch.linspace(0,1,height)\n",
    "    grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
    "    return torch.cat([grid_x.unsqueeze(0),grid_y.unsqueeze(0)], dim = 0).permute(1,2,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "config.perception_size = 3\n",
    "config.hierarchy_construct = [5,3,2]\n",
    "config.conv_feature_dim = 128\n",
    "model.scene_perception = ValkyrNet(config)\n",
    "\n",
    "perception_outputs = model.scene_perception(sample[\"image\"])\n",
    "scene_tree = perception_outputs[\"scene_tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "optimizer.zero_grad()\n",
    "working_loss = 0\n",
    "for key in perception_outputs[\"losses\"]: working_loss += perception_outputs[\"losses\"][key]\n",
    "working_loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_scene_tree(perception_outputs):\n",
    "    all_kwargs = []\n",
    "    scene_tree = perception_outputs[\"scene_tree\"]\n",
    "    scores = scene_tree[\"object_scores\"]\n",
    "    features = scene_tree[\"object_features\"]\n",
    "    connections = scene_tree[\"connections\"]\n",
    "\n",
    "    B = features[0].shape[0]\n",
    "    for b in range(B):\n",
    "        kw_scores, kw_features, kw_connections = [score[b] for score in scores], [feature[b] for feature in features], \\\n",
    "        [connection[b] for connection in connections]\n",
    "        kwargs = {\"features\":kw_features, \"end\":kw_scores, \"connections\":kw_connections}\n",
    "        all_kwargs.append(kwargs)\n",
    "    return all_kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_tree = perception_outputs[\"scene_tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = 1\n",
    "vis_scores = [score[b].detach() for score in scene_tree[\"object_scores\"][1:]]\n",
    "vis_connections = [connect[b] for connect in scene_tree[\"connections\"][1:]]\n",
    "\n",
    "\n",
    "visualize_tree(vis_scores, vis_connections, scale = 1.618)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16384, 5]) tensor(1., grad_fn=<MaxBackward1>) tensor(0., grad_fn=<MinBackward1>)\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([2, 16384, 3]) tensor(1., grad_fn=<MaxBackward1>) tensor(5.4651e-44, grad_fn=<MinBackward1>)\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<SumBackward1>)\n",
      "torch.Size([2, 16384, 2]) tensor(1., grad_fn=<MaxBackward1>) tensor(3.6743e-36, grad_fn=<MinBackward1>)\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for mask in perception_outputs[\"masks\"]:\n",
    "    print(mask.shape,mask.max(), mask.min())\n",
    "    print(torch.sum(mask, dim = 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tp/c2bzzvd17y35z8x_20mfhn700000gn/T/ipykernel_33112/63699579.py:47: MatplotlibDeprecationWarning: Auto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n",
      "  plt.subplot(1,batch_size,i + 1,frameon=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAACMCAYAAACEVee4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAIfElEQVR4nO3df+hddR3H8ecrLa2EnElkFmoq+KPMH9MCUQN1/oIpWKhhTjMGov2EyAgazSLNP4zA0mUjLVBLEAwtHf7qj1z5M39izinlKgw3hVC0zXd/3DO5ftv2uZvn+733O58PuOyczzmfy/vL3V4795zzPe9UFZK0Ke8YdwGSJp9BIanJoJDUZFBIajIoJDUZFJKaDApJTc2gSLI0yfNJHt3I9iT5cZIVSR5OcvDQtgVJnupeC/osXNLMGeWI4hfA8ZvYfgKwd/daCPwUIMlOwCLgk8BhwKIkc95KsZLGoxkUVfUHYPUmdjkZuKYGlgM7JtkFOA5YVlWrq2oNsIxNB46kCdXHOYpdgb8PrT/XjW1sXNIss+24CwBIspDB1xaAJVW1ZJR5u194s7+oMgOevfikjLsGjVcfQbEK+MjQ+oe7sVXAp6eM37WhN+iCYaRwkDTz+vjqcRNwVnf141PAS1X1T+BWYF6SOd1JzHndmKRZpnlEkeRaBkcGOyd5jsGVjHcCVNUVwC3AicAK4GXgnG7b6iQXAfd2b7W4qjZ1UlTShGoGRVWd0dhewPkb2bYUWLplpUmaFN6ZKanJoJDUZFBIajIoJDUZFJKaDApJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqWmkoEhyfJInu25gF25g+2VJHupef03y4tC2dUPbbuqxdkkzZJRnZm4DXA4cy6A3x71Jbqqqx9fvU1VfG9r/S8BBQ2/xSlUd2FvFkmbcKEcUhwErqmplVb0GXMegO9jGnAFc20dxkibDKEExcsevJLsBewB3DA1vn+S+JMuTnLKlhUoan75PZp4O3FBV64bGdququcDngB8l2XPqpCQLuzC5r+saJmmCjNIpbGOdwDbkdKY8ur+qVnV/rkxyF4PzF09P2cdOYdIEG+WI4l5g7yR7JHkXgzD4v6sXSfYB5gD3DI3NSbJdt7wzcDjw+NS5kibbKA2A1ia5gEE7wG2ApVX1WJLFwH1VtT40Tgeu6xoCrbcvcGWS1xmE0sXDV0skzQ5587/r2cVu5jPDbubyzkxJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNfXUKOzvJv4c6gn1xaNuCJE91rwV9Fi9pZvTSKaxzfVVdMGXuTsAiYC5QwP3d3DW9VC9pRkxHp7BhxwHLqmp1Fw7LgOO3rFRJ49Jnp7BTkzyc5IYk6/uAjNxlTNLk6utk5m+B3avqAAZHDVdvzmQ7hUmTbZSgaHYKq6oXqurVbvUq4JBR53bzl1TV3O5lxzBpwvTSKSzJLkOr84EnuuVbgXldx7A5wLxuTNIs0lensC8nmQ+sBVYDZ3dzVye5iEHYACyuqtXT8HNImkZ2ClOTncLknZmSmgwKSU0GhaQmg0JSk0EhqcmgkNRkUEhqMigkNRkUkpoMCklNBoWkJoNCUpNBIanJoJDUZFBIajIoJDUZFJKa+uoU9vUkj3eP6789yW5D29YNdRC7aepcSZOvr05hDwJzq+rlJOcBPwRO67a9UlUH9lu2pJnUS6ewqrqzql7uVpczeCy/pK1En53C1jsX+N3Q+vZdY5/lSU7Z/BIljVvzq8fmSHImg4bERw0N71ZVq5J8FLgjySNV9fSUeQuB9R3CltgESJosowTFSN2+khwDfBs4aqhrGFW1qvtzZZK7gIOANwVFFwyGgzSh+uoUdhBwJTC/qp4fGp+TZLtueWfgcGD4JKikWaCvTmGXAjsAv0kC8Leqmg/sC1yZ5HUGoXTxlKslkmaBkc5RVNUtwC1Txr4ztHzMRub9Efj4WylQ0vh5Z6akJoNCUpNBIanJoJDUZFBIajIoJDUZFJKaDApJTQaFpCaDQlKTQSGpyaCQ1GRQSGoyKCQ1GRSSmgwKSU0GhaSmvjqFbZfk+m77n5LsPrTtW934k0mO67F2STOkGRRDncJOAPYDzkiy35TdzgXWVNVewGXAJd3c/Rg8jHd/4HjgJ937SZpFeukU1q1f3S3fABydwVN2Twauq6pXq+oZYEX3fpJmkb46hb2xT1WtBV4C3j/iXEkTrtdOYVtqSzuFPXvxSZm+qqZHkoV2Qpu93q6f3yhHFKN0CntjnyTbAu8DXhhxLlW1pKrmdq+t/UNY2N5FE+xt+fn10imsW1/QLX8GuKOqqhs/vbsqsgewN/DnfkqXNFP66hT2c+CXSVYAqxmECd1+v2bQRnAtcH5VrZumn0XSNMngP37NlLfrd9ytxdv18zMoJDV5C7ekJoNCUpNBIanJoOhRksVJvjq0/v0kX0nyjST3Jnk4yXe7be9NcnOSvyR5NMlpYytcG5Rk9yRPJPlZkseS3Jbk3UkOTLK8+zxvTDJn3LVON4OiX0uBswCSvIPBZeJ/Mbh/5DDgQOCQJEcy+CW5f1TVJ6rqY8Dvx1KxWvYGLq+q/YEXgVOBa4BvVtUBwCPAovGVNzMMih5V1bPAC0kOAuYBDwKHDi0/AOzD4C/fI8CxSS5JckRVvTSeqtXwTFU91C3fD+wJ7FhVd3djVwNHjqOwmTQRv+uxlbkKOBv4IIMjjKOBH1TVlVN3THIwcCLwvSS3V9XimSxUI3l1aHkdsOOY6hgrjyj6dyODrxWHMrib9VbgC0l2AEiya5IPJPkQ8HJV/Qq4FDh4XAVrs7wErElyRLf+eeDuTey/VfCIomdV9VqSO4EXu9vVb0uyL3DP4BEd/Ac4E9gLuDTJ68B/gfPGVbM22wLgiiTvAVYC54y5nmnnnZk9605iPgB8tqqeGnc9Uh/86tGj7tF/K4DbDQltTTyikNTkEYWkJoNCUpNBIanJoJDUZFBIajIoJDX9Dz4g05pLsMyrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Visualize Predicate Segmentation]\n",
    "batch_kwargs = build_scene_tree(perception_outputs)\n",
    "#model.executor.effective_level = 3\n",
    "programs = [\"exist(scene())\",\"exist(filter(scene(),house))\"]\n",
    "for b in range(B):\n",
    "    kwargs = batch_kwargs[b]\n",
    "\n",
    "    q = programs[b]\n",
    "    q = model.executor.parse(q)\n",
    "    o = model.executor(q, **kwargs)\n",
    "    # [Visualize the Output Mask Scene Tree]\n",
    "    answer_distribution_binary(o[\"end\"].sigmoid().cpu().detach())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "programs = [\"scene()\",\"filter(scene(),house)\"]\n",
    "for b in range(B):\n",
    "    kwargs = batch_kwargs[b]\n",
    "    q = programs[b]\n",
    "    q = model.executor.parse(q)\n",
    "    o = model.executor(q, **kwargs)\n",
    "\n",
    "    # [Visualize the Output Mask Scene Tree]\n",
    "    vis_connections = [connect[b] for connect in scene_tree[\"connections\"][1:]]\n",
    "    vis_scores = [o[\"end\"][i].sigmoid().cpu().detach() for i in range(4)][1:]\n",
    "    \n",
    "    visualize_tree(vis_scores, vis_connections, scale = 1.618)\n",
    "    plt.show()\n",
    "\n",
    "#masks = calculate_masks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000000e+00 3.647637e-36]\n",
      "[1.0000000e+00 1.9849911e-39 1.1102012e-16]\n",
      "[8.5750202e-15 9.7670503e-43 1.6330765e-14 6.7408539e-25 1.0000000e+00]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABwCAYAAAC9zaPrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAF8ElEQVR4nO3dTWgdVRiH8WeS2irWtBUtumtARRBBXBSNG1Fxq0U3tyCCWnHvLntdiC7UlSK4EA240JULQXDjF1rUjaIg5hLpwoKtH0lraXOPi5NiPzK3N8mZ986dPD8YEibJzAkv98+575w7U6WUkCTFmBr3ACRpOzF0JSmQoStJgQxdSQpk6EpSoB3DfljBxUsb+n2YnW1yPKqRUqpKHauqKpestETJusIlr1lfr2NVV9tq2JKxy0J3MIDp6bIj00gM3W5qNHR9vY5VXW031l5YWioyGEkBfL220uihu7IC8/MNDkVSMb5eW2t46Pb7+S1Kvw9HjsDCQsigJG2Sr9fWG97TtffXGvZ0u6l4T9fatkaZnq4kaUsMXUkKZOhKUiBDV5ICGbqSFMjQlaRAhq4kBTJ0JSmQoStJgQxdSQpk6EpSIENXkgIZupIUyNCVpECGriQFMnQlKZChK0mBDF1JCmToSlIgQ1eSAhm6khTI0JWkQIauJAUydCUpkKErSYEMXUkKZOhKUiBDV5ICGbqSFMjQlaRAhq4kBTJ0JSmQoStJgQxdSQpk6EpSIENXkgIZupIUyNCVpECGriQFMnQlKZChK0mBuh26vR4sLsLqav7a6417RCrBumqSpZRqNyBN7NbrJZaXL/6Hlpfz/nGPbRPbsDptdBv3/2Jdm6nrxNe2Y1tdjaq1Qq2rqqr6H7bd4iIcOHD5/n4fZmejR7NlKaWq1LGsa3uUrCtMeG07pq62AaFbAXuBfcAx4MzWDzmK1VWYWqd7MhjA9HTMGAoydNdY16EmurYdU1fbgJ7ubuBx4FXgDmBH86cEWFra2H5NBuuqCddw6O4GHgMOAw8BzwBzwM5mTwswPw8rKxfvW1nJ+zW5rKsmXTNN+SrBngT3J/g2wakEgwQnEryd4M4Eu5pvZvd6icXFxOpq/jqhF1sY0pTflhdbrGt3a9uhLfhC2rXkGe7TwEHg6rX9CTgOfAa8CHwPDDZ3im3Gnm432dPtrsCe7h7gEPAEcC+w65Kf7wceBp4C7iOk1SBJLVHwqlYFzJBnts8Dt60dvrrkdyDPhA8D1wMngV+Af8sNRZJaqmDoXgM8Sp7h3s7/LYX1TJGXkT1Abi+8BhzFVoOkrisUujcDD5JbBnPAqOsl9wOPAKeA64CvgJWhfyFJk6xQ6N4KPAfcRQ7cUa4NXNhqOETu/f6MoSupywpdSOsDC8CP5BbBqBdQE7mX+wnwIfBXmeFIUksVmun+Bryz9n0iz3ivtCohkS+ifQG8AXwOnC0zHElqqYLrdCvy/RXmgJeAWXLLYL1WwwBYJs9wXwB+Ivd1Vcd1ut3kOt3uClinm4AT5Jnr68A3wLma3z0NfAC8CfyAgStpu2jg7jMngffIM9zd5JvcnG81JOBP8vKwt4CvsaUgaTtp6GPAFfmDDweBl4FbgKvILYWPgFfIM9zTVz7UFHnJb5O35jlHvuNki9+Y2V7oJtsL3TWm++nOAE8CPeBu4H3yLPhTRr6v7g3kFWU3bW0ktRJ5pdrHwN8NnaMAQ7ebDN3uqqttwze3/Qd4l7x29wx5lcJRNnQj8xuBZ8mfKm5CIl/P+5JWh66kbmg4dM8vC1sgJ9uvbPjJERV5EcRM4aGdNyB3PorONyRpfQGPcUjA72vbJv+8bhFEKedodT9XUnd0+xHsktQyhq4kBTJ0JSmQoStJgQxdSQpk6EpSIENXkgIZupIUyNCVpECGriQFMnQlKZChK0mBDF1JCmToSlIgQ1eSAhm6khTI0JWkQIauJAVqf+hGPLvM56NJCjIZobuz4XPsxOCVFCLgwZRbdBY4Duxt6PgJ+ANYbej4knSBKqX6x+BWVTX+Z+TOAPcA+xo6fgKOAd8Bpxo6RwEppWJz8VbUVUDZuoK1bZO62rY/dCG/9W/y7X+i9Y9gN3S7ydDtrrratr+9ABMRipI0ivZfSJOkDjF0JSmQoStJgQxdSQpk6EpSoKFLxiRJZTnTlaRAhq4kBTJ0JSmQoStJgQxdSQpk6EpSoP8A6tEdStmW8nAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABXCAYAAACnZJZlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEyklEQVR4nO3dv2skZRzH8fckJ56lIorijztBQUQrS8HqCm0sZW1FFLRSq6BiIaiNcJzgded/YK8odoeChXj9hhSC4hEPspyY7H4tJiFefkyy8+M7s5v3Cx4Cs9mZJx/Ch82zT3aKiECSlGOl7wlI0lli6UpSIktXkhJZupKUyNKVpESWriQlOlf1YAH7+8kiYMWOPigiirrPLYrC/XonqJvvHb+7sxmsrrY2p2XRSrZgvkeoyvb0LTqdtjIZKd3GRt8zWG7mO5fTlW4EXL3a8VSkDkwmsLbW9yyWl/nOLyKOHcxmwfZ2cOVKUP5J4TgwqvI7afQ990UYtbOdToPxOBiNev8ZhjpqZzseh/nWz7ao+jdg1xxP5pput2qvO5rticy2O+2s6UqSGrN0JSmRpStJiSxdSUpk6UpSIktXkhJZupKUyNKVpESWriQlsnQlKZGlK0mJLF1JSmTpSlIiS1eSElm6kpTI0pWkRJauJCWydCUpkaUrSYksXUlKZOlKUiJLV5ISWbqSlMjSlaRElq4kJbJ0JSmRpStJiSxdSUpk6UpSosUp3dEIxmOYTsuvo1HfM1oeZivliYhjBxCDGKNRsLV15+S2tsrjPc+tKr+TRt9zH3q2TfLte96LMMy2n2yL3RCPVBTF8Q9WKnav3ZLxGC5cOHx8fR0uXmzvOjVERFH3ufXzbdGAs4X6+Q4i24Ez2+5UZdtB6T4BPAX8APw7/9OPMp3CyhErIbMZrK62c42aFr50B5wtWAxdMtvuVGXb8pru48DbwGfAJeDudk67sTHfcZ2e2UqpWirdAngMeAd4HXgG+BR4ETjf/PRrazCZ3HlsMimPqxmzlXI1XzAvAh4OuBywGTALiIDtgBsBlwLON1+cHo2C8TiYTsuvC/5Gz6DekBhotk3y7XveizDMtp9sG67pFsADwIfACLh399ieHeAGsAZ8T2trvAOy8Gu6A+e6Y3fMtjsdremuAA8CHwGvcrhwAc4BzwIfAy/RylKDJC2wBqX7EGWZvgbcz+HC3bMKPA98ALxCWcSSdDY1KN1t4HfgH/aXMo4SwBS4CWxWfJ8kLb8GLzv/BC5TFuqbwCOUhfr/V7xBua77HeVuhuu73y9JZ1PDv/VvAV9SFu0bwKMHHt8GfgQ+AX7GwpV01rWwwLpXvDPgLcpXvAXlssN1yp0Lv2LhSlJr72r9DXxFuSXsPeA+4CfgXcotYztHP22VcgNE039cC+AvYKvheSSpYy1uJdgErlE26cvA+1QWLpSbHr4Anm546dvA58A3Dc8jSR1ref/WTeBr4FvgNyoLF+Au4EnguYaXnbC/TdjNEZIGrINNs3/sjlNqY6l31tJ5JKlji3PnCElaApauJCWydCUpkaUrSYksXUlKZOlKUiJLV5ISWbqSlMjSlaRElq4kJbJ0JSmRpStJiSxdSUrUf+kee3f4Oc/RxnkkqWP93g89KO/qc6vheW5TfryjJA1cEXH8p34XRdHtR4LfA7xAeXefJnaAX4D1phOaX0TUfo3deb5LoG6+Znsys+1OVbb9lu4SsHS7ZTF0x2y7U5Vt/2u6knSGWLqSlMjSlaRElq4kJap8I02S1C5f6UpSIktXkhJZupKUyNKVpESWriQlsnQlKdF/BFXBAkSs1MoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_masks(scores, connections):\n",
    "    all_masks = []\n",
    "    for i in range(len(connections)):\n",
    "        curr_mask = scores[i+1].unsqueeze(-1)\n",
    "        for connection in reversed(connections[:i+1]):\n",
    "            curr_mask = torch.bmm(connection, curr_mask) # [BxNxM],[BxM]\n",
    "        all_masks.append(curr_mask)\n",
    "    return all_masks\n",
    "\n",
    "vis_score = scene_tree[\"object_scores\"]\n",
    "#vis_score[-1][0][1] = 0.5\n",
    "print(vis_score[-1][0].cpu().detach().numpy())\n",
    "print(vis_score[-2][0].cpu().detach().numpy())\n",
    "print(vis_score[-3][0].cpu().detach().numpy())\n",
    "all_masks = calculate_masks(vis_score,scene_tree[\"connections\"])\n",
    "\n",
    "#for mask in all_masks:\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.imshow(mask[0].reshape(128,128).detach(), cmap = \"bone\")\n",
    "    #print(\"Max:\",float(mask[0].max()),\"Min:\",float(mask[0].min()))\n",
    "    #plt.show()\n",
    "\n",
    "def display_batch(sample,outputs,batch = 0,file_name = \"temp.png\"):\n",
    "    \n",
    "    B, W, H, C = sample[\"image\"].shape\n",
    "    for i, masks in enumerate(reversed(outputs[\"masks\"])):\n",
    "        plt.figure(\"display_{}\".format(i))\n",
    "        B, N, K = masks.shape\n",
    "        plt.subplot(1, K + 1, 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(sample[\"image\"][0])\n",
    "\n",
    "        layer_connection = outputs[\"scene_tree\"][\"edges\"][-i-1][batch].detach()\n",
    "        poses = outputs[\"poses\"][-i-1][\"centers\"][batch].detach()\n",
    "        plt.scatter(poses[:,0] * W, poses[:,1] * H, c = \"cyan\")\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                plt.plot(W * poses[i,:],H * poses[j,:],color = \"red\",\\\n",
    "                     alpha = 1)\n",
    "\n",
    "        for j in range(K):\n",
    "            plt.subplot(1, K + 1, 2 + j)\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(masks.detach()[batch][:,j].reshape(W,H), cmap=\"bone\")\n",
    "            plt.scatter(poses[j,0] * W, poses[j,1] * H, c = \"cyan\")\n",
    "        \n",
    "    return 0\n",
    "\n",
    "#plt.axis(\"off\")\n",
    "#mask = scene_tree[\"connections\"][0].reshape(2,128,128,5).detach()\n",
    "#N = 5\n",
    "#for i in range(N):\n",
    "#    plt.subplot(1,N,i + 1)\n",
    "#    plt.axis(\"off\")\n",
    "#    plt.imshow(mask[0,:,:,i])\n",
    "#plt.show()\n",
    "\n",
    "#plt.axis(\"off\")\n",
    "#plt.imshow(sample[\"image\"][0])\n",
    "\n",
    "display_batch(sample,perception_outputs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2298)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(2, 100, 2)\n",
    "attn = (torch.randn(2,5,100 ) ** 2).clamp(0.,1.0)\n",
    "\n",
    "local_loss = spatial_variance(x, attn)\n",
    "print(local_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0385)\n",
      "torch.Size([1, 1, 3, 2, 1])\n",
      "tensor(0.0633)\n"
     ]
    }
   ],
   "source": [
    "def assignment_entropy(s_matrix):\n",
    "    # s_matrix: B,N,M\n",
    "    EPS = 1e-6\n",
    "    output_entropy = 0\n",
    "    for b in range(s_matrix.shape[0]):\n",
    "        for i in range(s_matrix.shape[1]):\n",
    "            input_tensor = s_matrix[b][i:i+1,:].clamp(EPS, 1-EPS)\n",
    "\n",
    "            lsm = nn.LogSoftmax(dim = -1)\n",
    "            log_probs = lsm(input_tensor)\n",
    "            probs = torch.exp(log_probs)\n",
    "            p_log_p = log_probs * probs\n",
    "            entropy = -p_log_p.mean()\n",
    "            #print(entropy)\n",
    "            output_entropy += entropy\n",
    "\n",
    "    return output_entropy\n",
    "    \n",
    "\n",
    "def equillibrium_loss(att):\n",
    "    pai = att.sum(dim=3, keepdim=True) # B1K11\n",
    "    loss_att_amount = torch.var(pai.reshape(pai.shape[0], -1), dim=1).mean()\n",
    "    return loss_att_amount\n",
    "\n",
    "s_matrix = torch.tensor([\n",
    "    [0.5,0.4],\n",
    "    [0.2,0.2],\n",
    "    [0.3,0.4],\n",
    "]).unsqueeze(0)\n",
    "\n",
    "entropy_loss = assignment_entropy(s_matrix)\n",
    "print(entropy_loss)\n",
    "\n",
    "\n",
    "equis =s_matrix.unsqueeze(1).unsqueeze(-1)\n",
    "print(equis.shape)\n",
    "equi_loss = equillibrium_loss(equis)\n",
    "print(equi_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [1., 0.]])\n",
      "tensor([[0.7311, 0.2689],\n",
      "        [0.7311, 0.2689],\n",
      "        [0.2689, 0.7311]])\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "raw_edge = [\n",
    "    [],\n",
    "    [],\n",
    "    []\n",
    "]\n",
    "raw_size = 3\n",
    "coarse_size = \n",
    "\n",
    "S = torch.zeros(raw_size,2).float()\n",
    "\n",
    "raw_g = nx.Graph()\n",
    "raw_g.add_edges_from(raw_edge)\n",
    "\n",
    "A = torch.tensor([\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0]\n",
    "]).float()\n",
    "\n",
    "new_adj = torch.matmul(S.permute(1,0), torch.matmul(A, S))\n",
    "print(new_adj)\n",
    "\n",
    "normalize = torch.softmax(S, dim = 1)\n",
    "print(normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirs torch.Size([64, 64, 3])\n",
      "rays_d torch.Size([64, 64, 3])\n",
      "rays_o torch.Size([64, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "from utils.render import *\n",
    "\n",
    "\n",
    "H, W = (64, 64)\n",
    "K = ((1, 0, 2),\n",
    "     (0, 1, 2),\n",
    "     (0, 0, 1))\n",
    "\n",
    "c2w = torch.tensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "])\n",
    "\n",
    "rays_o, rays_d = get_rays(H, W, K, c2w, 0, 0, 0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a47e46093c771f9510c4aabf3710bfb1355e5f870a13f8c22092f45d4d23626d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Melkor')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
